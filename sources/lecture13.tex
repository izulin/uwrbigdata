\documentclass[11pt]{article}
\input{header.tex}

\newcommand{\E}{{\mathbb E}}
\DeclareMathOperator{\var}{Var}
\newcommand{\eps}{\varepsilon}
\setcounter{MaxMatrixCols}{20}

\begin{document}

\lecture{13: MPC}{ 01/06/2022}

\section{Massively Parallel Computing \cite{DBLP:conf/spaa/LattanziMSV11}}
Modern distributed computing model (captures map-reduce, hadoop, spark, etc.).

\begin{itemize}
\item input of size $n$
\item $k$ machines, each of space $S$, $S = n^{1-\delta}$, $k \cdot S = \bigo(n)$.
\item output (might be too large, e.g. size $n$ that does not fit on a single machine)
\end{itemize}

Computation:
\begin{itemize}
\item computation happens over $R$ rounds
\item each machine: near linear computation per round, so total computation cost $\bigo(n^{1+o(1)} R)$
\item each machine communicates $\sim S$ bits per round, so total communication cost $\bigo(n R)$
\end{itemize}
goal: minimize $R$

\subsection{Sorting (Tera-Sort)}
Intuition: if we partition input onto machines, so each machine receives contiguous fragment of size $S$, then we are done in a single round (each machine sorts and outputs its own part, output == concatenation of outputs).

Idea: 
\begin{itemize}
\item each machine receives input
\item each machine samples randomly from its input
\item each sample is sent to single machine ($1$ round)
\item 1 machine gathers all the samples, sorts locally, and sends back to everyone approximate histogram
\item machines use approximate histogram to decide how to partition locally their input and sent it to proper receivers
\item then everyone sorts their parts
\end{itemize}

Total sample size $s = \bigo(S)$, and whp histogram is with $\pm \varepsilon n$ error, where $\frac{\log n}{\varepsilon^2} \le s$. We are fine if error is $\bigo(S)$. This is satisfied when $S^{3/2} = \Omega(n \sqrt{\log n})$, or $S = \widetilde\Omega(n^{2/3})$, or $k = \widetilde\bigo(n^{1/3})$ $\implies$ sorting in $\bigo(1)$ rounds.


\section{Connectivity}
Input: list of edges, partitioned (arbitrarily), size $N$
output: each vertex labelled with id of connected component

Notation: id of vertex $\pi(v)$, label of vertex $\ell(v)$, $\Gamma(v)$ denotes neighbourhood
Approach:
\begin{itemize}
\item let $L_v$ be the set of vertices $\{u : \ell(u) = \ell(v)\}$ -- always a subset of connected component of $v$
\item initially $\ell(v) \gets \pi(v)$
\item some vertices are called \emph{active}
\item every $L_v$ will have exactly one active vertex (e.g. one with smallest $\ell$), that makes decisions wrt whole $L_v$, assume canonically its $v$
\item in each round:
\begin{itemize}
\item each active vertex becomes a \emph{leader} with ppb $1/2$
\item if $v$ is a leader, mark whole $L_v$ as a leaders
\item every active non-leader $v$, find $w \in \Gamma(L_v)$ that is a leader and minimizes $\pi(w)$
\item whole $L_v$ joins $L_w$
\end{itemize}
\end{itemize}

Observation: each round decreases \emph{in expectation} number of components by a factor of $1/4$, so $\bigo(\log n)$ rounds are enough. Implementation details:
\begin{itemize}
\item edges are stored locally
\item split vertices of large degree into smaller vertices of degree $\bigo(S)$ (this is already non-trivial and requires e.g. sorting)
\item distributed data structure implementing vertex info required (implemented e.g. via sorting, emulates sending messages over the edges etc.)
\end{itemize}


\section{MST}
Input: graph of $N = n^{1+\delta}$ edges. Output: some approximation of a maximal matching. Output fits into single machine (one edge per vertex). Assume $S = n^{1+\varepsilon}$.
\begin{itemize}
\item Split output randomly into machines.
\item Each machine $i$ computes MST of its input $T_i$
\item Recurse on $\bigcup_i T_i$
\end{itemize}

Filtering step reduces total number of edges from $n^{1+\delta}$ to $n^{1+\delta-\varepsilon}$, thus the number of rounds is $\bigo(\delta/\varepsilon)$.


\section{Maximal matching}
(Setting the same as in previous algorithm)

\begin{itemize}
\item Sample $E'$, set of edges of size $\bigo(S)$. 
\item Send $E'$ to one machine. Compute $M'$, the maximal matching of $E'$.
\item Remove all vertices from $V[E']$ from graph. Recurse on remaining graph. Call $M$ the result of recursion.
\item Return $M \cup M'$.
\end{itemize}

Correctness: simulated greedy. 
\begin{lemma}If $E'$ is a set of edges picked by sampling with probability $p$. Let $I$ be any set of vertices, with $E[I]$ being set of adjacent edges to $I$. Following holds with very high probability: either $E[I]$ is small (that is $E[I] \le 2n/p$), or $E[I]$ intersects with $E'$.
\end{lemma}
\begin{proof}
Fix any $I$. If $E[I]$ is not small, then probability of non-empty intersection is at least $1-(1-p)^{2n/p} \approx 1 - \exp(-2n)$. Taking union bound over all possible $2^n$ choices for $I$, we get that the claim holds with probability at least $1 - 2^n \exp(-2n)$.
\end{proof}
\begin{theorem}
Number of rounds is $\bigo(\delta/\varepsilon)$.
\end{theorem}
\begin{proof}
Let number of edges in the graph before filtering be $n^{1+\delta}$.
Fix sampling probability to be $p = S/m = n^{\varepsilon-\delta}$. The number of edges is reduced from $^{1+\delta}$ to $2n/p = 2n^{1+\delta-\varepsilon}$:
\begin{itemize}
\item let $I$ be unmatched vertices
\item set of edges is reduced to $E[I]$
\item if $E[I]$ is too large, then there is $e$ both in sampled edges and in $E[I]$
\item but then $e$ would have been selected for a matching edge.
\end{itemize}
\end{proof}

\section{Maximal matching, approach 2}
\begin{itemize}
\item Partition edges randomly into machines. 
\item Each machine receives $G_i$.
\item Each machine computes $M_i$, maximal matching of $G_i$.
\item Everyone sends $M_i$ to a coordinator machine.
\item Coordinator outputs $M$, maximal matching from $\bigcup_i M_i$.
\end{itemize}

The limiting factor is that all of the maximal-matchings need to be aggregated on a single machine, so $S \ge n \cdot \frac{N}{S}$, or $S \ge \sqrt{n \cdot N}$, which for dense graphs is $n^{3/2}$.

Claim: algorithm outputs maximal matching of size $\Theta(n)$, so its $\Theta(1)$-approximation of MM.

\begin{proof}
Consider greedy algorithm going through edges in order of $M_1,\ldots,M_k$. W.l.o.g. maximal matching is of size $\bigo(n)$. Consider step $i$ (processing $M_i$). 

\begin{lemma}
Assume we have already selected $o(n)$ edges from $M_1 \cup \ldots \cup M_{i-1}$, as otherwise we are done.\\
$E_{\text{old}}$ -- all edges in $G_i$ that are adjacent to already selected vertex\\
$\mu_{\text{old}}$ -- size of a maximum matching in $G_i$ using only edges in $E_{\text{old}}$.\\
There exists matching in $G_1 \cup \ldots \cup G_{i}$ of size $\mu_{\text{old}} + \Omega(n/k)$.
\end{lemma}
\begin{proof}
$G$ contains a matching of size $\Theta(n) - 2 \mu_{\text{old}} = \Theta(n)$ that is not adjacent to any of the vertices from already selected matching. By random partitioning, $\Theta(n/k)$ of those edges land in $G_i$, so $G_i$ contains matching of size $\mu_{\text{old}} + \Omega(n/k)$.
\end{proof}


So any maximal matching in $G_i$ needs to be of size at least $\mu_{\text{old}} + \Omega(n/k)$. At most $\mu_{\text{old}}$ of edges in any maximal matching can be adjacent to vertices already blocked in previous rounds, so from maximal matching we are getting $\Omega(n/k)$ new edges.
\end{proof}

\bibliographystyle{alpha}
\bibliography{bib}

\end{document}


