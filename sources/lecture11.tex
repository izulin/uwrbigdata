\documentclass[11pt]{article}
\input{header.tex}

\newcommand{\E}{{\mathbb E}}
\DeclareMathOperator{\var}{Var}
\newcommand{\eps}{\varepsilon}
\setcounter{MaxMatrixCols}{20}

\begin{document}

\lecture{11: Graph Algorithms}{ 18/05/2022}{Lecturer: \emph{Przemysław Uznański }}{-}


\section{Model}
Graph algorithms in sublinear memory/time. Need to define reasonable model, to avoid 'just run offline algorithm'.

Input model:
\begin{itemize}
\item Streaming of input
\item Semi-streaming (only edge insertions)
\item Oracle access to input (adjacency matrix + cell probe, adjacency lists)
\end{itemize}

Output model:
\begin{itemize}
\item Decision: is the graph bipartite?
\item Optimization: weight of MST
\item Sketch/summary/coreset (summary of input, e.g. mergeable sketches)
\item Local query model: does this vertex belong to MIS (needs to be consistent across independent runs)
\end{itemize}

Additional assumptions:
\begin{itemize}
\item Promise on the input (e.g. graph is connected)
\item Decision: bipartite or $\varepsilon$-far from bipartite (needs to flip $\varepsilon n^2$ edges to make it bipartite)
\item small memory (total)
\item small memory (per vertex)
\end{itemize}

\section{Cell probe MST}
Model: cell probe access to graph, given by adjacency lists. Problem: given graph $G$ with integer weights $\{1,\ldots,w\}$, find weight of MST. Assumption: max-degree is $d$, which is small.

\subsection{Counting connected components}
Simpler problem: how to count connected components (approximately) in unweighted graph?
Only hope for (fast) algorithm is additive $\pm \varepsilon n$ approximation.

\begin{observation}
To estimate the number of connected components in a graph $H$ we first pick a random vertex $v$.
If $v$ is in a large connected component, that is an indication of a small number of connected components.
\end{observation}

Denote $K$ as number of connected components. Let $\text{CC}(v)$ denote connected component of $v$.

\begin{lemma}
For $v$ let $\alpha_v = \frac{1}{|\text{CC}(v)|}$. Then $K = \sum_v \alpha_v$.
\end{lemma}
\begin{proof} (Simple.)
\end{proof}

Algorithm 1:
\begin{enumerate}
\item Sample $k$ vertices $v_1,\ldots,v_k$.
\item Find $\alpha_{v_1},\ldots,\alpha_{v_k}$.
\item Output $C = \frac{n}{k} \sum_{i} \alpha_{v_i}$.
\end{enumerate}

Issues: how large $k$ needs to be? Computing $\alpha_{v_i}$ might take $\mathcal{O}(n)$ steps.

$$\E[C] = \frac{n}{k} \sum_{i} \E[\alpha_{v_i}] = \frac{n}{k} \cdot k \cdot \frac{K}{n} = K$$
$$\mathrm{Var}[C] \le \frac{n^2}{k^2} \sum_i \E[(\alpha_{v_i})^2] \le  \frac{n^2}{k^2} \sum_i \E[\alpha_{v_i}] = \frac{n^2}{k^2} \cdot k \cdot \frac{K}{n} = \frac{n}{k} K \le \frac{n^2}{k}$$
so the additive error is with e.g. $8/9$ ppb at most $3\sqrt{\mathrm{Var}[C]} \le  3n/\sqrt{k}$. So it is enough to set $k = \bigo(\frac{1}{\varepsilon^{2}})$.

What about other issue of DFS/BFS taking too long on large connected components? We truncate the BFS/DFS after at most $A = \frac{1}{\varepsilon}$ steps. So large CC's are reported to be of size $A$. This introduces additive error of $\pm \frac{1}{A}$ each $\alpha$, so $\pm \varepsilon n$ to actual output of algorithm.

Total runtime:
$\bigo(\frac{1}{\varepsilon^2} \cdot \frac{1}{\varepsilon} \cdot d)$.

Amplify success to whp: repeat $\bigo(\log n)$ times and take the median.

\subsection{MST from connected components}
Let $G_1,G_2,\ldots,G_w$ be unweighted graphs such that: $e \in G_i$ iff $w(e) \le i$. Denote by $K_i$ the number of connected components of $G_i$.

\begin{theorem}
Weight of MST satisfies
$$w(\text{MST}) = (n-1) + \sum_{i=1}^{w-1} (K_i - 1)$$
\end{theorem}
\begin{proof} (Simple.)
\end{proof}


If we run each MST estimator with error $\pm (\varepsilon/w)n$ (runtime $\bigo(\frac{d w^4 \log n}{\varepsilon^3})$), the total error of estimation is $\pm \varepsilon n$. Observation: since $w(\text{MST}) \ge n-1$, this is $1\pm \bigo(\varepsilon)$ multiplicative error.


\section{Graph sketching for MST \cite{DBLP:conf/pods/AhnGM12}}
\subsection{$L_0$ sampling}
We consider a following problem:
\begin{definition}
Maintain a multiset $M$ over universe $[n]$, under insertions and deletions, and queries for random element. Random element query returns any $x \in M$ with probability $\sim \frac{1}{\|M\|_0}$, that is any unique element from $M$ with roughly the same probability.
\end{definition}

\paragraph{Note:} random element query needs to be random when queried once. Consecutive queries might be fully correlated.

We are interested in a solution that takes $\text{poly} \log n$ space.

\paragraph{First} assume we have a guarantee that when there is a query, $\|M\|_0 = 1$ (the trick is that $M$ might grow large and then shrink). The solution is to maintain:
\begin{itemize}
\item $C = \sum_{x \in M} x$
\item $D = \sum_{x \in M} x^2$
\end{itemize}
and to output $\frac{D}{C}$.

\paragraph{Generalizing:} Now, to generalize to arbitrary size. If we know the $\|M\|_0 \sim k$ for some guessed value $k$, we can pick a hash function $h : [n] \to [k]$ and care only about $x$ such that $h(x) = 0$. That is:
\begin{itemize}
\item $C_k = \sum_{x \in M} \mathbf{1}[h(x) = 0] \cdot x$
\item $D_k = \sum_{x \in M} \mathbf{1}[h(x) = 0] \cdot x^2$
\end{itemize}
\paragraph{Decoding} if actually one element hashed: return $\frac{D_k}{C_k}$.  If $k \le \|M\|_0 \le 2k$, then there is constant probability that there is single value $h(x) = 0$ (since this happend with ppb $1/k$ for each element). 
To detect failure, we can change the scheme a little bit: 
\begin{itemize}
\item pick hash function $g : [n] \to [n]$, and maintain $C'_k = \sum_{x \in M} \mathbf{1}[h(x) = 0] \cdot x \cdot g(x)$ and $D'_k = \sum_{x \in M} \mathbf{1}[h(x) = 0] \cdot x^2 \cdot g(x)$ instead of $C_k$ and $D_k$
\item after we decode $x$ and $r_x$, number of repetitions of $x$ in $M$, we verify values $C'_k$ and $D'_k$
\end{itemize}
There are other ways to detect failures, e.g. add $F_0$ sketch to the scheme, or maintain sketches for more powers $x,x^2,\ldots,x^p$ for some small $p$.

General scheme:
\begin{itemize}
\item Maintain independently $\log n$ levels, each responsible for $k=1,2,\ldots,2^{\log n}$.
\item On each level, maintain $\log n$ independent repetitions. For correct level, each repetition is ok with constant probability, we need just one to work.
\end{itemize}
Total size is poly-logarithmic.

\subsection{Connected components}
We want to sketch $G$ to maintain connected components of $G$ under edge insertions and deletions. The sketch size of $\widetilde\bigo(n)$ -- this is necessary since initially each vertex is in its own connected component.

Sketch of algorithm:
\begin{itemize}
\item Initialize each vertex with separate connected component (sketch).
\item Proceed in rounds: 
\begin{itemize}
\item in each round, each connected component picks at random one incident edge
\item all components connected by edges are merged
\end{itemize}
\end{itemize}

\begin{lemma}
If $K$ is actual number of connected components, and $K_i$ denotes number after round $i$, there is $K_{i+1} - K \le \frac{K_i - K}{2}$.
\end{lemma}
Thus, $\log n$ rounds are enough.

\paragraph{Edge-based sketching:} we orient arbitrarily each edge, labelling its endpoints with $-1$ and $+1$.
Then, with each vertex $v$ we associate function $E \to \{-1,0,1\}$: $v(e) = 1$ or $v(e) = -1$ if $v$ is an endpoint of $e$, and otherwise $v(e) = 0$.

For each connected component $X \subseteq V$ we maintain $L_0$ sampler for multiset set:
$$X(e) = \sum_{v \in X} v(e)$$

For any edge, it is present in the multiset $X$ only iff its one endpoint is in $X$ and other endpoint is in $V \setminus X$. Thus $L_0$ sampling over $X$ gives us any adjacent edge.

The $L_0$ sampler presented in previous subsection is actually mergeable, since all functions there were linear.

\subsection{MST}
Recall: 
$$w(\text{MST}) = (n-1) + \sum_{i=1}^{w-1} (K_i - 1)$$
so we keep connected components sketch separately for each edge weight.

However, since we can get precise values of $K_i$, we can do better -- we can round each edge weight down to nearest power of $(1+\varepsilon)$. This introduces $1 \pm \varepsilon$ factor, but reduces number of different edge weights to $\bigo(\frac{\log w}{\varepsilon})$.

Also note that since our sketches are linear, any edge insertion and removal can be done on the go. Total space is $\bigo(\frac{\log w \cdot \text{poly} \log n}{\varepsilon})$ per vertex, and processing time per insert/removal is the same, and query time is $\bigo(n \cdot \frac{\log w \cdot \text{poly} \log n}{\varepsilon})$ 

\bibliographystyle{alpha}
\bibliography{bib}

\end{document}

