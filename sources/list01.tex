\documentclass[12pt]{uebung}


\dozent{Przemysław Uznański}
\vorlesung{Algorithms for Big Data}
\semester{Fall Semester 2019}
%\tutoren{Tutoren name}
 
\usepackage{amsmath}
\usepackage{mathtools}
 
 
%\usepackage[utf8]{inputenc}
\usepackage{amsmath, amssymb,wasysym}
%\usepackage{tikz}
\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\usepackage[english]{babel}
 
%\usepackage{multirow}


 \begin{document}

 \startnummer{1}
 
\kopf[0]{07/10/2019}{1}


\renewcommand{\aufgname}{Exercise}





In the following set we are concerned in designing a (memory/query) efficient algorithm for a following problem: we are given (in an offline form\footnote{offline: read-only}) a binary array $A[1 .. n]$ where $\forall_i A[i] \in \{0,1\}$. Our goal is to estimate (up to some additive error $\varepsilon$) the value of $Y = \frac{1}{n}\sum_i A[i]$ using only little additional memory.

\begin{aufg}[2 pts]
Show that simple \emph{random sampling} performs well: select independently $i_1,i_2,\ldots,i_k \in [n]$. Show that $\frac{1}{k}(A[i_1]+\ldots+A[i_k])$ is an unbiased estimator of $Y$.\footnote{$X$ is an \emph{unbiased estimator} of $Y$ iff $\mathbb{E}[X] = Y$.} Use Hoeffding bound to bound $k$, the number of samples necessary, so that the estimation holds:
\begin{itemize}
\item with probability $9/10$,
\item with probability $1-1/n$? (So called \emph{with high probability}.)
\end{itemize}
\end{aufg}

\begin{aufg}[2 pts]
Use Chebyshev's inequality (instead of Hoeffding bound) to bound $k$ from Exercise 1. How many samples do we need so that the estimation holds:
\begin{itemize}
\item with probability $9/10$,
\item with probability $1-1/n$?
\end{itemize}
\end{aufg}

\begin{aufg}[2 pts]
Consider $9/10$ probability estimation from previous exercise. Consider $t$ fully independent repetitions of the same estimation procedure, with values $Y_1,Y_2,\ldots,Y_t$. Show that for $t=\Theta(\log n)$, the value of $\textrm{median}(Y_1,\ldots,Y_t)$ is an $\pm \varepsilon$ estimation of $Y$ with high probability.
What is the total number of samples needed?
\end{aufg}

\begin{aufg}[2 pts]
\begin{itemize}
\item Prove Markov's inequality. 
\item Show that Chebyshev's inequality follows from Markov's inequality.
\end{itemize}
\end{aufg}

\newpage

\begin{theorem}[Markov's inequality]
Let $X \ge 0$ be a random variable. Then for any $k\ge1$:
$$\Pr( X \ge k \cdot \mathbb{E}[X] ) \le \frac{1}{k}.$$
\end{theorem}

\begin{theorem}[Chebyshev's inequality]
Let $X$ be a random variable. For any $k > 0$:
$$\Pr( |X - \mathbb{E}[X]| \ge k \cdot \sqrt{\mathrm{Var}[X]} ) \le \frac{1}{k^2}. $$
\end{theorem}

\begin{theorem}[Hoeffding bound]
Let $X_1,X_2,\ldots,X_n \in \{0,1\}$ be \textbf{fully independent random variables}. Let $X = \sum_i X_i$. Then:

$$\Pr( |X - \mathbb{E}[X]| \ge t) \le 2 \exp(- \frac{t^2}{n}).$$
\end{theorem}

\end{document}
